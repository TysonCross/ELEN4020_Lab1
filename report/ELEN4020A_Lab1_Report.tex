%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                   ELEN4020A Lab 1 Report
%                                 Tyson Cross       1239448
%                                 Michael Nortje    1389486
%                                 Josh Isserow      675720
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[10 pt, conference]{cssconf}
\IEEEoverridecommandlockouts
\overrideIEEEmargins
\setlength{\columnsep}{0.66cm}
%-------------------------------------------------------------------------------------
%	External LaTeX files
%-------------------------------------------------------------------------------------
\input{tex/packages.tex}
\input{tex/custom_commands.tex}
\input{tex/title.tex}
%-------------------------------------------------------------------------------------
%	DOCUMENT BEGIN
%-------------------------------------------------------------------------------------
\pagestyle{plain}
\begin{document}
\twocolumn[
\begin{@twocolumnfalse}
\maketitle\thispagestyle{plain}
%-------------------------------------------------------------------------------------
%	TITLE SECTION
%-------------------------------------------------------------------------------------
\maketitle\thispagestyle{plain}
%-------------------------------------------------------------------------------------
%	ABSTRACT
%-------------------------------------------------------------------------------------
\begin{center}\begin{minipage}{1\textwidth} \vspace{-0.3cm}
\begin{abstract}
Abstract goes here \vspace{2pt}
\end{abstract}\end{minipage}\end{center}\vspace{0.2cm}
\end{@twocolumnfalse}]
%-------------------------------------------------------------------------------------
%	MAIN REPORT CONTENTS
%-------------------------------------------------------------------------------------
\section*{Introduction}
\begin{itemize}
    \item Introduction to Tensors as generalised higher dimensional vectors/matrices/arrays.
    \item Use of Python. 
    \item Use of Numpy, and numpy arrays
    \item Access methods used (shape in particular)
    \item For loop and non-parallelism of the code.
    \item Motivation and laboratory exercises objectives.
    \item Python argument passing mechanism
    \item Assumptions
    \item Definitions
    
    Citations to include: \cite{hackbusch2012tensor} \cite{Hunt-notes} \cite{sochi2016introduction} \cite{sringeri2015tensor}
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{section}{0}
\section{Rank 2 Tensor addition}
The function \verb|rank2TensorAdd()| was implemented with the method shown in psuedocode in Algorithm \ref{pc:rank2_add}. The function performs element-wise addition on two input rank 2 tensors $\mathbf{A}$ and $\mathbf{B}$, returning a new rank 2 tensor $\mathbf{C}$ as output. The function takes in three arguments: two 2d matrices (numpy arrays) and a single integer value \textit{N}. A square 2d matrix (same number of rows and columns) is equivalent to a rank 2 tensor with both dimensions equal in the number of elements. In the implemented function, the assumption is made that both input tensors must be square and of rank 2. This requirement is checked by passing both input matrices to the function \verb|checkRank2valid()| which uses the in-built Python \verb|assertion| method to confirm separately that both$\mathbf{A}$ and $\mathbf{B}$ are square and a third assertion checking that both input 2d Numpy arrays are the same shape. If any of the assertions fail (i.e. a required condition is not met) then the script execution is halted and an error string describing the failure is printed to the terminal output.
If the assertions inside the error-checking method pass without error, then the method continues. The first step is initialising a new rank 2 tensor as a 2-dimensional Numpy array, with all elements set to zero. Two nested for-loops are then iterated through for the both dimensional indices (\textit{i,j}) in $\mathbf{A}$ and $\mathbf{B}$, performing element-wise addition as shown in equation \ref{eq:sum_2d}:

\begin{equation}\label{eq:sum_2d}
    c_{ij} \mathrel{{+}{=}}
     a_{ij} + b_{ij}
\end{equation}

This operation is identical to standard element-wise summing of two matrices.

\begin{algorithm}[ht]
\caption{Rank 2 Tensor addition}\label{pc:rank2_add}
    	\SetAlgoLined
     	\KwIn{tensor A (N x N), tensor B (N x N), N}
    	\KwOut{tensor C (N x N)}
	Initialize C to zero\;
	\For{i = 1 to N}{
		\For{j = 1 to N}{
			$C_{ij}  \leftarrow A_{ij} + B_{ij}$\
		}
	}
\end{algorithm}

\subsection{Error checking}
The 2d addition function  assumes that the input tensors are of the same length in all dimensions, and equal in dimensions. The two inputs are passed to simple error-checking function which uses python assertions to check these requirements. A message is printed in case of failure, as the script halts.This function is called \verb|checkRank2Valid(A,B)|.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Rank 2 Tensor Multiplication}
The function \verb|rank2TensorMult()| performs multiplication of two input 2D matrices A and B. The same validity check is performed using the \verb|checkRank2valid()| function to ensure both input rank 2 tensors $\mathbf{A}$ and $\mathbf{B}$ are square and of the same shape.
Each element of matrix C is initialised to zero. The matrix product $\mathbf{AB}$) is calculated using the summation of element-wise row and column multiplications, as shown in equation \ref{pc:rank2_mult}:

\begin{equation}\label{eq:product_2d}
    c_{ij} \mathrel{{=}}
     \sum_{i=1}^{N}\sum_{j=1}^{N} a_{ik}b_{kj}
\end{equation}

The implementation is shown as pseudocode in Algorithm \ref{pc:rank2_mult}. The function uses nested for-loops to perform the summations across equated indices of the row and columns of the input matrices, respectively. The innermost loop's individual row-column product is summed with the last iteration of the current element of C being calculated, with the result then stored as as the new value of the current element in C. This requires initialising the C matrix, with all elements set zeros before performing the iteration loops, to avoid accidentally adding the random contents from in the allocated memory location for the array.

\begin{algorithm}[ht]
\caption{Rank 2 Tensor Multiplication}\label{pc:rank2_mult}
    	 \SetAlgoLined
     	\KwIn{tensor A (N x N), tensor B (N x N), N}
    	\KwOut{tensor C (N x N)}
	Initialize C to zero\;
	\For{i = 1 to N}{
		\For{j = 1 to N}{
			\For{k = 1 to N}{
				$C_{ij}  \leftarrow C_{ij} + A_{ij} * B_{ij}$\
			}		
		}
	}
\end{algorithm}

\subsection{Error checking}
The 2d multiplication function also calls the \verb|checkRank2Valid(A,B)| function to confirm that the input matrices are square.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Rank 3 Tensor Addition}

\subsection{rank3TensorAdd()}
\begin{algorithm}[ht]
\caption{Rank 3 Tensor Addition}\label{pc:rank3_add}
    	 \SetAlgoLined
     	\KwIn{tensor A (N x N x N), tensor B (N x N x N), N}
    	\KwOut{tensor C (N x N x N)}
	Initialize C to zero\;
	\For{i = 1 to N}{
		\For{j = 1 to N}{
			\For{k = 1 to N}{
				$C_{ijk}  \leftarrow A_{ijk} + B_{ijk}$\
			}
		}
	}
\end{algorithm}

\subsection{Error checking}
The 3d addition function also assumes that the input tensors are of the same length in all dimensions, and equal in dimensionality. It passes the two inputs to a simple error-checking function which checks these requirements with python assertion statements, which halt script execution and print a message to output in case of failure. This function is called \verb|checkRank3Valid(A,B)|.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Rank 3 Tensor Multiplication}

\begin{figure}[ht] \centering 
    \vspace*{-5pt}
    \small{\includegraphics[width=1\linewidth]{images/rank3_tensor_mult.eps}}
    \small\caption{Implemented rank 3 Tensor multiplication}
    \label{fig:tensor_product} 
\end{figure}%

Tensor multiplication has two broad categories: the outer product, which is well defined, and the inner product, which  is often an application-specific operation. The required specification for the multiplication operation for the laboratory was to multiply two 3d matrices (rank 3 tensors) with the same number of elements, and produce a new 3d matrix.

\subsection{Tensor Contraction}
Tensor contraction involves equating two or more of the input tensor dimensional indices, referred to the "free variable(s)", and iterating through the remaining indices (known as the "dummy variables") in element-wise row-column product assignment. An example of single contraction between a rank 4 tensor $\mathbf{A[N][N][N][N]}$ and rank 2 tensor $\mathbf{B[N][N]}$, using Einstein summation notation (which drops the implied sigma symbol in the summation operation when there is a repeated index), results in a rank 4 tensor $\mathbf{C}[N][N][N]$, as shown in equation \ref{eq:tensor_contraction_single}:

\begin{equation}\label{eq:tensor_contraction_single}
    c_{ijkm} \mathrel{{=}} a_{ijkx}b_{xm}
\end{equation}

One of the indices in the first tensors has been equated ("contracted") with an index in the second tensor. Any definition of contraction results in a reduction of dimensions, in this case with the resultant tensor having a rank of $(\textit{n}-2)$, where \textit{n} is the sum of the dimensions of both matrices.  Double contraction would involve equating two of each tensors' indices making the formula for dimensional reduction $(\textit{n - 2r})$, where $r=1$ for single contraction, $r=2$ for double contraction, etc. However, this definition of an inner tensor product does not meet the requirements of the laboratory. Applying this operation to the inner product of two 3d matrices would result in a general formulation: $\mathbf{A}[N][N][N]\mathbf{B}[N][N][N]=\mathbf{C}[N][N][N][N]$ which possesses an additional dimension. The implementation of the laboratory function for multiplying two square 3d arrays is constructed differently.

\subsection{rank3TensorMult()}
The implementation of the function \verb|rank3TensorMult()| is shown in Algorithm \ref{pc:rank3_mult}. A graphical explanation of the operation is shown in Figure \ref{fig:tensor_product}. The function takes two cubic matrices as inputs, along with \textit{N}, where \textit{N} is the length of all three dimensions in both input matrices. The function first initialises $\mathbf{C}$ to zero for all elements. Then a single dimension in both input tensors is equated (contracted), and a "slice" of the 2d matrix at this index in the contracted 3rd dimension is extracted from both inputs. The resulting two 2d matrices are then multiplied (by normal matrix multiplication), performing contraction again, in the form of element-wise dot-product for each element in the product matrix. This matrix is also 2-dimension. The resulting matrix is then assigned to a "slice" of $\mathbf{C}$, at the index in the 3rd dimension corresponding to the original contraction index. This operation "stacks" the 2d matrix which is the product of the extracted input slices, resulting in a cubic tensor which has the same dimensions and total number of elements as both of the inputs.

\begin{algorithm}[ht]
\caption{Rank 3 Tensor Multiplication}\label{pc:rank3_mult}
    	 \SetAlgoLined
     	\KwIn{tensor A (N x N x N), tensor B (N x N x N), N}
    	\KwOut{tensor C (N x N x N)}
	Initialize C to zero\;
	\For{x = 1 to N}{
        $C_{ijx}  \leftarrow C_{ijx} + \big[A_{ik}B_{kj}\big]_{x}$\
	}
\end{algorithm}

\subsection{Error checking}
The 3d multiplication function makes the same assumptions of the input matrix length and dimensionality, and calls the same error-checking function called before 3d addition, \verb|checkRank3Valid(A,B)|.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\addtolength{\textheight}{-11.0cm} 
% This command serves to balance the column lengths
% on the last page of the document manually. It shortens
% the textheight of the last page by a suitable amount.
% This command does not take effect until the next page
% so it should come on the page before the last. Make
% sure that you do not shorten the textheight too much.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
Conclusion goes here.

% -------------------------------------------------------------------------------------
% 	BIBLIOGRAPHY
% ------------------------------------------------------------------------------------
\clearpage
\newpage
\onecolumn
\bibliographystyle{IEEEtran}
{\bibliography{IEEEabrv,ELEN4020A_Lab1_Report.bib}}

\input{tex/appendices.tex}

\end{document}